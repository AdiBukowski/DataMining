\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % dodatkowe pakiety LaTeX'a
\usepackage[OT4]{polski}
\usepackage[cp1250]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{animate}
\newtheorem{theorem}{Twierdzenie}
\usepackage{mathtools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ustawienia globalne
<<ustawienia_globalne, echo=FALSE, warning=FALSE,results='hide',message=FALSE>>=
library(knitr)
library(xtable) #pakiet do tworzenia tabel w formacie LaTeX'a
library(animation)
library(ElemStatLearn)
library(MASS)
library(cluster)
library(ggplot2)
library(ROCR)
library(stats)
library(e1071)
library(neuralnet)
library(randomForest)
library(mlr)
library(kknn)


opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=5, fig.height=4,message=FALSE)
# UWAGA: w razie potrzeby mo¿na zmieniaæ te ustawienia w danym chunk'u!
@
  
  
  
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% strona tytulowa
\title{Filtrowanie spamu}
\author{Adrian Bukowski  Anna Miko³ajczyk}
\maketitle
\tableofcontents 

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wstêp oraz analiza opisowa}
Naszym zadaniem jest rozpoznanie czy dany email jest spamem czy nie. Mamy do czynienia z 57 zmiennymi spoœród których:
\begin{itemize}
\item 48 przyjmuje wartoœci od 0 do 100 i oznacza procentowy udzia³ danego s³owa w mailu (za s³owo uznajemy dowolny ci¹g liter i cyfr)
\item 6 przyjmuje wartoœci od 0 do 100 i oznacza procentowy udzia³ danego znaku w mailu 
\item 1 przyjmuje wartoœci dodatnie i oznacza œredni¹ d³ugoœæ nieprzerywanych ci¹gów wielkich liter
\item 1 przyjmuje wartoœci naturalne i oznacza d³ugoœæ najd³u¿szego nieprzerwanego ci¹gu wielkich liter
\item 1 przyjmuje wartoœci naturalne i oznacza ³¹czna iloœæ wielkich liter w mailu
\end{itemize}
Oprócz tego dane zawieraj¹ kolumnê spam/mail oznaczaj¹c¹ czy dany mail jest spamem. Mamy wiêc do czynienia z zadaniem klasyfikacji. Zbiór zawiera 4601 elementów. Przyjrzyjmy siê danym:
<<>>=
data("spam")
dim(spam)
X<-spam[,1:(dim(spam)[2]-1)]
y<-spam[,dim(spam)[2]]
@
SprawdŸmy czy dane s¹ dobrze wczytane:
<<>>=
sapply(X,class)
@
Jak widaæ zmienne s¹ zgodne z opisem. Zobaczmy czy nie wystêpuj¹ brakuj¹ce dane:
<<>>=
sum(is.na(X))
@
Nie mamy do czynienia z danymi brakuj¹cymy.

Podstawowe statystyki dla zmiennych objaœniaj¹cych:
<<>>=
summary(X)
@
Zobaczmy tak¿e jak wygl¹da kowiariancja zmiennych:
<<>>=
kowariancja <- cov(X)
heatmap(kowariancja)
@

Przyjrzyjmy siê tak¿e zmiennej objaœnianej:
<<>>=
class(y)
levels(y)
@
Jak widaæ jest to zmienna typu "factor" o dwóch poziomach - "email" i "spam".

Zobaczmy jak dziel¹ siê nasze dane ze wzglêdu na zmienn¹ obj¹snian¹:
<<>>=
tab<-table(y)
tab
as.matrix(tab)
data<-as.data.frame(tab)
colnames(data)<-c('type','freq')
ggplot(data, aes(x=as.factor(type), y=freq,fill=as.factor(type)))+geom_bar(stat = "identity")+ labs(x="", y='Ilosc') + theme(legend.position="none")
prop.table(tab)
@
Jak widaæ oko³o 60\% to emaile, a 40\% to spam.

W celu bardziej œwiadomej analizy dodajmy nazwy kolumn:
<<>>=
spamColNames <- c("word_freq_make", "word_freq_address", "word_freq_all", "word_freq_3d", 
    "word_freq_our", "word_freq_over", "word_freq_remove", "word_freq_internet", 
    "word_freq_order", "word_freq_mail", "word_freq_receive", "word_freq_will", 
    "word_freq_people", "word_freq_report", "word_freq_addresses", "word_freq_free", 
    "word_freq_business", "word_freq_email", "word_freq_you", "word_freq_credit", 
    "word_freq_your", "word_freq_font", "word_freq_000", "word_freq_money", 
    "word_freq_hp", "word_freq_hpl", "word_freq_george", "word_freq_650", "word_freq_lab", 
    "word_freq_labs", "word_freq_telnet", "word_freq_857", "word_freq_data", 
    "word_freq_415", "word_freq_85", "word_freq_technology", "word_freq_1999", 
    "word_freq_parts", "word_freq_pm", "word_freq_direct", "word_freq_cs", "word_freq_meeting", 
    "word_freq_original", "word_freq_project", "word_freq_re", "word_freq_edu", 
    "word_freq_table", "word_freq_conference", "char_freq_ch;", "char_freq_ch(", 
    "char_freq_ch[", "char_freq_ch!", "char_freq_ch$", "char_freq_ch#", "capital_run_length_average", 
    "capital_run_length_longest", "capital_run_length_total")
colnames(X)<-spamColNames
@

<<>>=
X.spam<-X[y=='spam',1:48]
X.email<-X[y=='email',1:48]
avg.X.spam <- sort(sapply(X.spam,mean),decreasing = TRUE)
avg.X.email <- sort(sapply(X.email, mean),decreasing = TRUE)
avg.X.spam<-as.data.frame(avg.X.spam[1:10])
colnames(avg.X.spam)<-'avg_word_freq'
avg.X.email<-as.data.frame(avg.X.email[1:10])
colnames(avg.X.email)<-'avg_word_freq'
ggplot(avg.X.spam, aes(x=reorder(rownames(avg.X.spam),-avg_word_freq),y=avg_word_freq))+geom_bar(stat = "identity")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(avg.X.spam, aes(x=reorder(rownames(avg.X.email),-avg_word_freq),y=avg_word_freq))+geom_bar(stat = "identity")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
@

<<>>=
X.spam<-X[y=='spam', 49:54]
X.email<-X[y=='email', 49:54]
X.spam <- sort(sapply(X.spam, mean),decreasing = TRUE)
X.email <- sort(sapply(X.email, mean),decreasing = TRUE)
X.spam <- cbind(X.spam,'spam')
X.email <- cbind(X.email,'email')
colnames(X.spam) <- c('avg.char.freq','type')
colnames(X.email) <- c('avg.char.freq','type')
X.char.freq <- rbind(X.spam,X.email)
X.char.freq <- cbind(rownames(X.char.freq),X.char.freq)
colnames(X.char.freq) <- c('char', 'avg.char.freq', 'type')
X.char.freq <- as.data.frame(X.char.freq) 
ggplot(X.char.freq, aes(x=as.factor(char), y = avg.char.freq, fill=type))+geom_bar(stat = "identity",position = "dodge")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
@

<<>>=
par(mfrow=c(1,3))
col.55<-as.data.frame(as.matrix(by(X[,55],y,mean)))
col.56<-as.data.frame(as.matrix(by(X[,56],y,mean)))
col.57<-as.data.frame(as.matrix(by(X[,57],y,mean)))
ggplot(col.55, aes(x=rownames(col.55),y=V1,fill=rownames(col.55)))+geom_bar(stat='identity') + theme(legend.position="none")
ggplot(col.56, aes(x=rownames(col.56),y=V1,fill=rownames(col.56)))+geom_bar(stat='identity') + theme(legend.position="none")
ggplot(col.57, aes(x=rownames(col.57),y=V1,fill=rownames(col.57)))+geom_bar(stat='identity') + theme(legend.position="none")
@

\section{Klasyfikacja}

<<>>=
task <- makeClassifTask(data = spam, target = "spam")
cv_folds <- makeResampleDesc("CV", iters = 5)
@

\subsection{Regresja Logistyczna}
<<>>=
logistic.learner <- makeLearner("classif.logreg",predict.type = "prob")
cv.logistic <- crossval(learner = logistic.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.logistic$aggr
ROC(cv.logistic$pred$data[,4],cv.logistic$pred$data[,2])
@

\subsection{LDA}
<<>>=
lda.learner <- makeLearner("classif.lda",predict.type = "prob")
cv.lda <- crossval(learner = lda.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.lda$aggr
ROC(cv.lda$pred$data[,4],cv.lda$pred$data[,2])
@

\subsection{Klasyfikator Naiwny Baysa}
<<>>=
bayes.learner <- makeLearner("classif.naiveBayes", predict.type = 'prob')
cv.bayes <- crossval(learner = bayes.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.bayes$aggr
ROC(cv.bayes$pred$data[,4],cv.bayes$pred$data[,2])
@

\subsection{Metoda k-Najbli¿szych S¹siadów}
<<>>=
knn.learner <- makeLearner("classif.kknn", predict.type = 'prob')
cv.knn <- crossval(learner = knn.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.knn$aggr
ROC(cv.knn$pred$data[,4],cv.knn$pred$data[,2])
@

\subsection{Sieci neuronowe}
<<>>=
neuralnet.learner <- makeLearner("classif.neuralnet", predict.type = 'prob')
cv.neuralnet <- crossval(learner = neuralnet.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.neuralnet$aggr
ROC(cv.neuralnet$pred$data[,4],cv.neuralnet$pred$data[,2])
@

\subsection{Lasy losowe}
<<>>=
randForest.learner <- makeLearner("classif.randomForest", predict.type = 'prob')
cv.randForest <- crossval(learner = randForest.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.randForest$aggr
ROC(cv.randForest$pred$data[,4],cv.randForest$pred$data[,2])
@

\subsection{Maszyna wektorów noœnych}
<<>>=
classif.svm 
svm.learner <- makeLearner("classif.svm", predict.type = 'prob')
cv.svm <- crossval(learner = svm.learner, task = task,iters = 5,stratif=TRUE,measures = list(acc,auc),show.info = F)
cv.svm$aggr
ROC(cv.svm$pred$data[,4],cv.svm$pred$data[,2])
@


\section{Klasyfikacja}
\subsection{Metoda kNN}

\subsection{Regresja Logistyczna}
<<>>=
#regresja logistyczna
lr.spam <- glm(y~as.matrix(X),family = binomial(link='logit'))
summ.lr.spam <- summary(lr.spam)
tab1<-table(lr.spam$fitted.values>0.5,y)
sum(diag(tab1))/sum(tab1)
#regresja logistyczna ze wspó³czynnikami istotnymi statystycznie 
significant <- summ.lr.spam$coefficients[2:dim(summ.lr.spam$coefficients)[1],4]<0.05
X.sig <- X[,significant]
lr.spam.sig <- glm(y~as.matrix(X.sig), family = binomial('logit'))
summ.lr.spam.sig <- summary(lr.spam.sig)
tab2<-table(lr.spam.sig$fitted.values>0.5,y)
sum(diag(tab2))/sum(tab2)
@
Krzywe ROC
<<>>=
ROC<-function(pred.prob,true.labels){
  pred.ROCR <- ROCR::prediction(pred.prob, true.labels)
  perf.ROCR <- performance(pred.ROCR, "tpr", "fpr")
  plot(perf.ROCR, print.cutoffs.at=seq(0.1,1,0.1), colorize=TRUE, lwd=2)
  return(performance(pred.ROCR, "auc")@y.values[1])
}
ROC(lr.spam$fitted.values,y)
ROC(lr.spam.sig$fitted.values,y)
@

\subsection{Klasyfikator naiwny Bayesa}
<<>>=
smple<-sample(dim(spam)[1],0.2*dim(spam)[1])
test.set<-spam[smple,]
train.set<-spam[-smple,]

model<-naiveBayes(spam~.,data=train.set)
naiveBayes.prediction<-predict(model,test.set)

@


\subsection{Sieci neuronowe}



<<>>=
data<-spam
data$spam<-ifelse(data$spam=='spam',1,0)
maxs <- apply(data, 2, max) 
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
index<-sample(dim(spam)[1],0.2*dim(spam)[1])
train_ <- scaled[-index,]
test_ <- scaled[index,]
n<-names(spam)
f<- as.formula(paste('spam~',paste(n[!n %in% 'spam'],collapse = ' + ')))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
@

<<fig.cap='Sieæ neuronowa z dwoma warstwami ukrytymi.'>>=
plot(nn)
@


<<>>=
prediction.nn <- compute(nn,test_[,-dim(test_)[2]])
pred.test<-ifelse(prediction.nn$net.result>0.5,'spam','email')
tbl<-table(pred.test,test_$spam)
sum(diag(tbl))/sum(tbl)
ROC(prediction.nn$net.result,test_$spam)
@

\subsection{Lasy losowe}
<<>>=
model<-randomForest(spam~.,data=spam, subset = -index)
rand.forest.prediction<-predict(model,test_,type = 'prob')
ROC(rand.forest.prediction[,2],test_$spam)
rand.forest.prediction.labels<-predict(model,test_,type = 'response')
table(rand.forest.prediction.labels,test_$spam)
@


\section{Klasyfikacja z redukcj¹ wymiaru}
\subsection{Regresja Logistyczna}
\subsection{Klasyfikator naiwny Bayesa}
\subsection{Lasy losowe}

\section{Analiza skupieñ}
\subsection{K-means}
<<>>=
k <- 2
kmeans.k3 <- kmeans(X,centers=k,iter.max=10)
X.kmeans.clusters <- kmeans.k3$cluster
table(X.kmeans,y)
X.pca <- prcomp(X,retx=T)
X.pca.data <- X.pca$x[,1:2] 
X.pca.data.scaled<-scale(X.pca.data)
plot(X.pca.data.scaled[,1],X.pca.data.scaled[,2], col=as.factor(y), pch= as.numeric(X.kmeans.clusters))

k <- 3
kmeans.k3 <- kmeans(X,centers=k,iter.max=10)
X.kmeans <- kmeans.k3$cluster
table(X.kmeans,y)

@

\end{document}